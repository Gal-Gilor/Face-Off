{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon - Brain Tumor Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib as jb\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image, ImageFile\n",
    "from src.utils import train_image_classifier, test_image_classifier\n",
    "# the following import is required for training to be robust to truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Structure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "├── test\n",
    "│   ├── glioma.jpg\n",
    "│   ├── meningioma.jpg\n",
    "│   ├── no_tumor.jpg\n",
    "│   └── pituitary.jpg\n",
    "├── train\n",
    "│   ├── glioma\n",
    "│   ├── meningioma\n",
    "│   ├── notumor\n",
    "│   └── pituitary\n",
    "└── valid\n",
    "    ├── glioma\n",
    "    ├── meningioma\n",
    "    ├── notumor\n",
    "    └── pituitary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Data Loaders\n",
    "\n",
    "* Training DataLoader\n",
    "    * Transforms\n",
    "    * Normalize\n",
    "    * Image Augmentation\n",
    "* Validation DataLoader\n",
    "    * Transforms\n",
    "    * Normalize\n",
    "* Testing DataLoader\n",
    "    * Transforms\n",
    "    * Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train contains 5392 images\n",
      "Dataset valid contains 320 images\n",
      "Dataset test contains 1311 images\n"
     ]
    }
   ],
   "source": [
    "# define hyper parameters\n",
    "DATA_DIR = 'data/'\n",
    "TRAIN = 'train'\n",
    "VAL = 'valid'\n",
    "TEST = 'test'\n",
    "KERNEL_SIZE = (5, 9)\n",
    "SIGMA = (0.1, 2.5)\n",
    "SHARPNESS = 2\n",
    "ROTATION = 30\n",
    "RESIZE = (256, 256)\n",
    "CROP = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "\n",
    "data_transforms = { \n",
    "    TRAIN: transforms.Compose([ # define train set augmentations\n",
    "        transforms.Resize(RESIZE),\n",
    "        transforms.RandomCrop(CROP),\n",
    "        transforms.RandomVerticalFlip(), # default .5 probability\n",
    "        transforms.RandomRotation(ROTATION),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.GaussianBlur(KERNEL_SIZE, sigma=SIGMA),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=SHARPNESS), # default .5 probability\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    VAL: transforms.Compose([ # no need to perform any augmentation on the validation data\n",
    "        transforms.Resize(RESIZE),\n",
    "        transforms.CenterCrop(CROP),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    TEST: transforms.Compose([ # no need to perform any augmentation on the test data\n",
    "        transforms.Resize(RESIZE),\n",
    "        transforms.CenterCrop(CROP),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(DATA_DIR, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "# create data loaders dictionary\n",
    "loaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=BATCH_SIZE,\n",
    "        shuffle=True, num_workers=0  # turn on shuffle (though not needed for testing and validation)\n",
    "    ) \n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "# sanity check\n",
    "for dataset in image_datasets:\n",
    "    n_images = len(image_datasets[dataset])\n",
    "    print(f'Dataset {dataset} contains {n_images} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate, and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# use pre-trained EfficientNet\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# freeze intermediate layers\n",
    "for param in model.features.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "# remove last layer\n",
    "classifier_block = model.classifier\n",
    "\n",
    "num_features = model.classifier[-1].in_features # save the number of in features in the last layer\n",
    "classifier_block = list(classifier_block[:-1]) # remove last layer\n",
    "\n",
    "# replace the layer with a new output with the number of classes\n",
    "classifier_block.extend([nn.Linear(num_features, 4)])\n",
    "\n",
    "# replace classifier layer\n",
    "model.classifier = nn.Sequential(*classifier_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch 336 out of 337\tClassified correctly: 0.83902\n",
      "Epoch: 1 \tTraining Loss: 0.473401 \tValidation Loss: 0.460537 \t Training accuracy: 0.839021\n",
      "\n",
      "Validation loss improved from inf to 0.46053702804351154\n",
      "Epoch: 2\tBatch 336 out of 337\tClassified correctly: 0.93157\n",
      "Epoch: 2 \tTraining Loss: 0.196969 \tValidation Loss: 0.188067 \t Training accuracy: 0.931565\n",
      "\n",
      "Validation loss improved from 0.46053702804351154 to 0.18806735373500064\n",
      "Epoch: 3\tBatch 336 out of 337\tClassified correctly: 0.94955\n",
      "Epoch: 3 \tTraining Loss: 0.143909 \tValidation Loss: 0.140950 \t Training accuracy: 0.949555\n",
      "\n",
      "Validation loss improved from 0.18806735373500064 to 0.14095024231901446\n",
      "Epoch: 4\tBatch 336 out of 337\tClassified correctly: 0.96402\n",
      "Epoch: 4 \tTraining Loss: 0.110224 \tValidation Loss: 0.105531 \t Training accuracy: 0.964021\n",
      "\n",
      "Validation loss improved from 0.14095024231901446 to 0.10553073459256511\n",
      "Epoch: 5\tBatch 336 out of 337\tClassified correctly: 0.97033\n",
      "Epoch: 5 \tTraining Loss: 0.091308 \tValidation Loss: 0.087770 \t Training accuracy: 0.970326\n",
      "\n",
      "Validation loss improved from 0.10553073459256511 to 0.08777044232732976\n",
      "Epoch: 6\tBatch 336 out of 337\tClassified correctly: 0.97626\n",
      "Epoch: 6 \tTraining Loss: 0.073817 \tValidation Loss: 0.074168 \t Training accuracy: 0.976261\n",
      "\n",
      "Validation loss improved from 0.08777044232732976 to 0.07416846625683406\n",
      "Epoch: 7\tBatch 336 out of 337\tClassified correctly: 0.98053\n",
      "Epoch: 7 \tTraining Loss: 0.060334 \tValidation Loss: 0.058331 \t Training accuracy: 0.980527\n",
      "\n",
      "Validation loss improved from 0.07416846625683406 to 0.05833089601119648\n",
      "Epoch: 8\tBatch 336 out of 337\tClassified correctly: 0.98294\n",
      "Epoch: 8 \tTraining Loss: 0.054051 \tValidation Loss: 0.051398 \t Training accuracy: 0.982938\n",
      "\n",
      "Validation loss improved from 0.05833089601119648 to 0.05139750705064497\n",
      "Epoch: 9\tBatch 336 out of 337\tClassified correctly: 0.98553\n",
      "Epoch: 9 \tTraining Loss: 0.041080 \tValidation Loss: 0.040295 \t Training accuracy: 0.985534\n",
      "\n",
      "Validation loss improved from 0.05139750705064497 to 0.0402950756292963\n",
      "Epoch: 10\tBatch 336 out of 337\tClassified correctly: 0.98776\n",
      "Epoch: 10 \tTraining Loss: 0.041407 \tValidation Loss: 0.042475 \t Training accuracy: 0.987760\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 11\tBatch 336 out of 337\tClassified correctly: 0.98906\n",
      "Epoch: 11 \tTraining Loss: 0.033136 \tValidation Loss: 0.031502 \t Training accuracy: 0.989058\n",
      "\n",
      "Validation loss improved from 0.0402950756292963 to 0.03150225783535598\n",
      "Epoch: 12\tBatch 336 out of 337\tClassified correctly: 0.98869\n",
      "Epoch: 12 \tTraining Loss: 0.038499 \tValidation Loss: 0.036774 \t Training accuracy: 0.988687\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 13\tBatch 336 out of 337\tClassified correctly: 0.98943\n",
      "Epoch: 13 \tTraining Loss: 0.033793 \tValidation Loss: 0.032206 \t Training accuracy: 0.989429\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 14\tBatch 336 out of 337\tClassified correctly: 0.98776\n",
      "Epoch: 14 \tTraining Loss: 0.035924 \tValidation Loss: 0.034599 \t Training accuracy: 0.987760\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 15\tBatch 336 out of 337\tClassified correctly: 0.99221\n",
      "Epoch: 15 \tTraining Loss: 0.025177 \tValidation Loss: 0.023996 \t Training accuracy: 0.992211\n",
      "\n",
      "Validation loss improved from 0.03150225783535598 to 0.023995538722804126\n",
      "Epoch: 16\tBatch 336 out of 337\tClassified correctly: 0.99036\n",
      "Epoch: 16 \tTraining Loss: 0.030238 \tValidation Loss: 0.028748 \t Training accuracy: 0.990356\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 17\tBatch 336 out of 337\tClassified correctly: 0.99277\n",
      "Epoch: 17 \tTraining Loss: 0.022978 \tValidation Loss: 0.021923 \t Training accuracy: 0.992767\n",
      "\n",
      "Validation loss improved from 0.023995538722804126 to 0.021922906726885526\n",
      "Epoch: 18\tBatch 336 out of 337\tClassified correctly: 0.98924\n",
      "Epoch: 18 \tTraining Loss: 0.028041 \tValidation Loss: 0.026654 \t Training accuracy: 0.989243\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 19\tBatch 336 out of 337\tClassified correctly: 0.99184\n",
      "Epoch: 19 \tTraining Loss: 0.023842 \tValidation Loss: 0.025891 \t Training accuracy: 0.991840\n",
      "Validation loss hasn't improved. Model not saved\n",
      "\n",
      "Epoch: 20\tBatch 336 out of 337\tClassified correctly: 0.99369\n",
      "Epoch: 20 \tTraining Loss: 0.016984 \tValidation Loss: 0.016156 \t Training accuracy: 0.993694\n",
      "\n",
      "Validation loss improved from 0.021922906726885526 to 0.016156454150627286\n"
     ]
    }
   ],
   "source": [
    "# multiclass classification - PyTorch implemntation of CrossEntropyLoss applies Softmax and NLLLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer typically outperforms other optimizers (true for the time this was written)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "\n",
    "# train the model\n",
    "losses = train_image_classifier(EPOCHS, loaders, model, optimizer, criterion, device, 'tumor_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class_names_dict.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create class dictionary to access labels more easily later\n",
    "## list of class names by index, i.e. a name can be accessed like class_names[0]\n",
    "class_names = [item for item in image_datasets['train'].classes]\n",
    "class_names = {i: class_name for i, class_name in enumerate(class_names)}\n",
    "jb.dump(class_names, 'class_names_dict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Test Results\": {\n",
      "    \"Loss\": 0.015551043674349785,\n",
      "    \"Accuracy\": \"99.619%\",\n",
      "    \"Rcall\": \"99.6%\",\n",
      "    \"Precision\": \"99.7%\",\n",
      "    \"F1\": \"99.6%\",\n",
      "    \"Correct\": 1306,\n",
      "    \"Total\": 1311\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy \n",
    "model_testing = torch.load('tumor_classifier.pt')\n",
    "metrics = test_image_classifier(loaders[\"test\"], model_testing, criterion, device)\n",
    "print(json.dumps(metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.016983803757470703, 0.016156454150627286)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "total_loss = np.array(total_loss)\n",
    "plt.plot(total_loss.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(total_loss.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
